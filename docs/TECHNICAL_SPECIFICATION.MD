### **Техническое Задание: Конвейер пакетного импорта промптов (Версия 3.0)**

**Версия:** 3.0
**Дата:** 05.08.2025
**Автор:** AI-Архитектор

#### **1. Введение**

**1.1. Цель документа**

Настоящее ТЗ описывает требования и архитектуру локального **полуавтоматического конвейера (pipeline)** на языке Python. Цель конвейера — обеспечить надежный, гибкий и удобный способ массового импорта промптов из сохраненного HTML-кода веб-страниц в виде стандартизированных JSON-файлов, полностью соответствующих схеме проекта.

**1.2. Стратегическое решение**

Конвейер реализован в виде **единого, управляемого через конфигурацию офлайн-инструмента (`importer.py`)**, который работает с локальными HTML-файлами. Это устраняет зависимость от защитных механизмов сайтов и фокусируется на основной задаче — качественном, многоуровневом извлечении и структурировании данных.

**1.3. Пользовательский сценарий (Workflow)**

1.  **Шаг 1 (Ручной):** Пользователь сохраняет полный HTML-код страницы в соответствующий файл в папке `input/` (`index.html` для списка постов, `post.html` для отдельного поста).
2.  **Шаг 2 (Автоматический):** Пользователь из командной строки запускает скрипт, указывая нужный режим работы:
    *   Анализ списка: `python src/importer.py --mode index`
    *   Парсинг поста: `python src/importer.py --mode post`
3.  **Шаг 3 (Интерактивный):** В режиме `post` скрипт выполняет глубокий эвристический анализ HTML, извлекает данные (включая автора, ID, вложения) и предлагает пользователю их верифицировать и отредактировать через интерактивный CLI.
4.  **Шаг 4 (Результат):** После подтверждения скрипт генерирует готовый к коммиту `.json` файл в папку `output/` по заданной схеме.

---

#### **2. Технологический Стек и Архитектура**

*   **Язык:** Python 3.8+
*   **Основные зависимости:**
    *   `beautifulsoup4` + `lxml`: Для парсинга HTML.
    *   `rich`: Для создания интерактивных CLI-интерфейсов.
    *   `requests`: Для скачивания текстовых вложений.
*   **Ключевые компоненты архитектуры:**
    *   **`importer.py`**: Основной исполняемый модуль, содержащий класс `OfflineImporter`.
    *   **`importer_config.json`**: Центральный конфигурационный файл, описывающий пути, CSS-селекторы и правила. Управляет поведением скрипта без изменения кода.
    *   **`argparse`**: Модуль стандартной библиотеки для управления режимами запуска (`--mode`).

---

#### **3. Функциональные Требования**

##### **3.1. Конфигурационное управление (`importer_config.json`)**

Скрипт полностью управляется через внешний JSON-файл, который определяет:
*   Пути к входным файлам для каждого режима (`input_files`).
*   Путь к выходной директории (`output_dir`).
*   CSS-селекторы для каждого режима (`selectors`), что позволяет гибко адаптироваться к разной структуре страниц индекса и поста.

##### **3.2. Режимы работы**

*   **Режим "Индекс" (`--mode index`)**
    *   **Назначение:** Анализ HTML-файла, содержащего список ссылок на посты (`input/index.html`).
    *   **Логика:** Используя селекторы из секции `selectors.index_mode`, находит и выводит в консоль нумерованный список всех ссылок на посты с их описаниями.

*   **Режим "Пост" (`--mode post`)**
    *   **Назначение:** Глубокий анализ HTML-файла, содержащего один пост (`input/post.html`).
    *   **Логика:**
        1.  **Поиск контейнера поста:** Находит главный контейнер поста (например, `<table data-post="...">`) по селектору из `selectors.post_mode.post_container`.
        2.  **Двухэтапный поиск автора:**
            *   **Основной:** Ищет имя и ID автора в теге `span.normalname > a` внутри главного контейнера.
            *   **Запасной:** Если основной автор не найден, ищет автора последней правки в блоке `span.edit`.
        3.  **"Умный" эвристический парсер:**
            *   Извлекает заголовок, описание, дату последнего редактирования.
            *   Находит все вложения (`a.attach-file`), группирует их по имени.
            *   **Приоритетная загрузка:** В каждой группе отдает приоритет `.txt` файлу. Если он есть, скачивает его содержимое.
            *   **Формирование вариантов:** Создает `prompt_variants`: каждый скачанный файл или, в качестве альтернативы, каждый спойлер на странице становится отдельным вариантом промпта.
            *   **Очистка HTML:** Преобразует базовую HTML-разметку (`<br>`) в чистый текст с переносами строк.
        4.  **Интерактивный CLI-валидатор:** Выводит все извлеченные данные (включая ID автора и список вариантов) и позволяет пользователю подтвердить их или отредактировать `title` и `description`.
        5.  **Генератор JSON:** После подтверждения формирует **полностью валидный JSON-объект**, соответствующий финальной схеме проекта, и сохраняет его в `output/`.

---
---

### Анализ текущего состояния и возможные улучшения

Теперь, как аналитик, я проведу обзор созданной системы, выделю ее сильные/слабые стороны и предложу следующие шаги.

#### **Введение и ключевые понятия**

Мы создали высокоспециализированный пайплайн для преобразования данных. Его работа основана на трех концепциях:

1.  **Эвристический парсинг:** Вместо жестко заданных правил, система использует набор "догадок" (эвристик) для поиска данных в полуструктурированном HTML. Например: "автор, скорее всего, находится в `span.normalname`, а если нет, то в `span.edit`".
2.  **Конфигурационное управление:** Логика поиска данных (CSS-селекторы) вынесена из кода в JSON-файл. Это позволяет адаптировать парсер к изменениям верстки сайта без вмешательства в программный код.
3.  **Интерактивная валидация:** Признавая, что автоматика может ошибаться, система включает человека в цикл для окончательной проверки и коррекции данных, что гарантирует 100% качество на выходе.

#### **Сравнительный анализ системы**

| Преимущества / Сильные стороны                                  | Недостатки / Риски                                               |
| :-------------------------------------------------------------- | :--------------------------------------------------------------- |
| **Надежность и стабильность** (офлайн-режим)                      | **"Хрупкость" селекторов:** Любое изменение верстки сайта потребует ручного обновления `importer_config.json`. |
| **Высокое качество данных** (извлечение ID, версий, автора)       | **Полностью ручной первый шаг:** Процесс "скопировать HTML -> сохранить файл" является узким местом и не масштабируется. |
| **Гибкость и адаптивность** (управление через JSON-конфиг)        | **Ограниченная очистка HTML:** Сохраняется только текст. Форматирование (жирный, курсив, списки) теряется. |
| **Пользовательский контроль** (интерактивный редактор)           | **Отсутствие пакетной обработки:** Система работает только с одним файлом за раз, что неудобно при импорте 10+ постов. |

#### **Нюансы и скрытые аспекты (Возможные улучшения)**

*   **[Улучшение №1] Конвертация в Markdown вместо текста.**
    *   **Проблема:** Сейчас функция `_clean_html_to_text` сохраняет только голый текст, теряя важное форматирование.
    *   **Решение:** Заменить ее на более умный конвертер, который будет преобразовывать `<b>` в `**жирный**`, `<i>` в `*курсив*`, `<ul>/<li>` в списки Markdown. Это можно сделать с помощью библиотеки `html2text` или написав свою простую функцию замены. Это значительно повысит читаемость и ценность итогового контента.

*   **[Уелучшение №2] Пакетный режим (`--mode batch`).**
    *   **Проблема:** Запускать скрипт для каждого файла неудобно.
    *   **Решение:** Добавить новый режим, который будет автоматически находить и обрабатывать все `.html` файлы из указанной директории (например, `input/batch/`). Это позволит пользователю сохранить 10 постов и обработать их одной командой.

*   **[Уелучшение №3] Автоматическая категоризация и тегирование с помощью LLM.**
    *   **Проблема:** Поля `category` и `tags` сейчас заполняются вручную или по умолчанию.
    *   **Решение:** После парсинга текста передавать его локальной LLM (Ollama) с промптом: "Проанализируй этот текст и предложи одну наиболее подходящую категорию из списка [...] и 3-5 релевантных тегов". Результат можно предложить пользователю в интерактивном валидаторе как значение по умолчанию.

*   **[Уелучшение №4] Валидация по JSON Schema.**
    *   **Проблема:** Если мы ошибемся при формировании итогового `json_output`, ошибка обнаружится только при импорте в основную систему.
    *   **Решение:** Создать формальный файл JSON Schema, описывающий структуру финального объекта. Перед сохранением файла выполнять его валидацию с помощью библиотеки `jsonschema`. Это гарантирует, что на выход пайплайна всегда поступают только 100% валидные JSON-файлы.

#### **Заключение**

Текущая версия пайплайна (`v3.0`) является мощным и надежным инструментом, полностью решающим поставленную задачу по извлечению и структурированию данных. Его главная сила — в гибкости конфигурации и интеллектуальных эвристиках парсинга. Основным ограничением остается ручной характер сбора исходных HTML-файлов. **Следующим логическим шагом развития должно стать усиление автоматизации:** внедрение пакетной обработки и использование LLM для автоматического заполнения метаданных (категории, теги).