### **Техническое Задание: Конвейер пакетного импорта промптов (Версия 4.0)**

**Версия:** 4.0
**Дата:** 06.08.2025
**Автор:** AI-Архитектор

#### **1. Введение**

**1.1. Цель документа**

Настоящее ТЗ описывает требования и архитектуру локального **гибридного конвейера (pipeline)** на языке Python. Цель конвейера — автоматизировать сбор HTML-кода веб-страниц и обеспечить надежный, гибкий и удобный способ массового импорта промптов в виде стандартизированных JSON-файлов.

**1.2. Стратегическое решение**

Конвейер реализован в виде **двухкомпонентной системы**:

1.  **Модуль Сбора Данных (`scraper.py`):** Онлайн-инструмент, отвечающий за автоматизированную и "стелс" загрузку HTML-кода с целевого сайта. Его задача — имитировать поведение человека для обхода защитных механизмов.
2.  **Модуль Обработки и Валидации (`importer.py`):** Основной офлайн-инструмент, который принимает полученный HTML, выполняет глубокое эвристическое извлечение данных, проводит интерактивную валидацию с пользователем и генерирует итоговые JSON-файлы.

Такое разделение позволяет сочетать преимущества автоматизации с надежностью и качеством ручной верификации.

**1.3. Пользовательский сценарий (Workflow)**

1.  **Шаг 1 (Автоматический Сбор):** Пользователь из командной строки запускает модуль сбора данных, указывая URL темы и количество страниц для анализа:
    *   `python src/scraper.py --url "https://..." --pages 5`
2.  **Шаг 2 (Автоматическая Обработка):** Скрипт `scraper.py` последовательно загружает каждую страницу, имитируя поведение пользователя (с задержками и корректными заголовками). Для HTML-кода **каждого найденного поста** он вызывает логику модуля `importer.py`.
3.  **Шаг 3 (Интерактивная Валидация):** Для каждого поста модуль `importer.py` выполняет глубокий эвристический анализ, извлекает все данные и предлагает пользователю их верифицировать и отредактировать через интерактивный CLI.
4.  **Шаг 4 (Результат):** После подтверждения пользователем скрипт генерирует готовый к коммиту `.json` файл в папку `output/` для каждого обработанного поста.

---

#### **2. Технологический Стек и Архитектура**

*   **Язык:** Python 3.8+
*   **Основные зависимости:**
    *   `selenium`: Для управления браузером и "стелс"-сбора данных.
    *   `beautifulsoup4` + `lxml`: Для парсинга HTML.
    *   `rich`: Для создания интерактивных CLI-интерфейсов.
    *   `requests`: Для скачивания текстовых вложений.
*   **Ключевые компоненты архитектуры:**
    *   **`scraper.py` (Новый компонент):** Основной исполняемый модуль для онлайн-сбора данных. Содержит класс `StealthScraper`.
    *   **`importer.py`**: Модуль обработки и валидации. Содержит класс `OfflineImporter`, логика которого теперь вызывается из `scraper.py`.
    *   **`importer_config.json`**: Центральный конфигурационный файл, расширенный для управления обоими модулями.
    *   **`argparse`**: Модуль для управления параметрами запуска `scraper.py` (`--url`, `--pages` и т.д.).

---

#### **3. Функциональные Требования**

##### **3.1. Конфигурационное управление (`importer_config.json`)**

Конфигурационный файл расширяется для поддержки нового модуля:
*   **`scraper_settings` (Новая секция):**
    *   `random_delay_range`: Диапазон случайной задержки между запросами (например, `[3, 7]` секунд).
    *   `user_agents`: Список реалистичных User-Agent для ротации.
    *   `page_load_timeout`: Максимальное время ожидания загрузки страницы.
*   **`selectors`:** Секция остается, но теперь используется модулем `importer.py` для парсинга HTML, полученного от `scraper.py`.

##### **3.2. Модуль Сбора Данных (`scraper.py`)**

*   **Назначение:** Автоматизация сбора исходного HTML-кода с минимальным риском блокировки.
*   **Логика:**
    1.  **"Стелс"-режим:**
        *   При инициализации выбирает случайный `User-Agent` из списка в `importer_config.json`.
        *   Между загрузками страниц делает случайную паузу в диапазоне, указанном в `random_delay_range`.
        *   Использует продвинутые опции Selenium для маскировки автоматизации.
    2.  **Пакетный сбор:** Принимает базовый URL темы и количество страниц для обхода. Итеративно формирует URL для каждой страницы (`&st=0`, `&st=20` и т.д.).
    3.  **Извлечение ссылок на посты:** На каждой странице-индексе находит все ссылки на отдельные посты, используя селектор из `selectors.index_mode`.
    4.  **Передача на обработку:** Для каждой найденной ссылки (или для каждого поста на странице) загружает полный HTML поста и передает его в виде строки в соответствующий метод модуля `importer.py`.

##### **3.3. Модуль Обработки и Валидации (`importer.py`)**

*   **Назначение:** Глубокий анализ HTML, интерактивная валидация и генерация JSON.
*   **Изменения в логике:**
    *   Основной класс `OfflineImporter` теперь имеет публичный метод (например, `process_html(html_string: str)`), который принимает строку с HTML и запускает весь внутренний процесс парсинга и валидации.
    *   Режимы (`--mode index`, `--mode post`), управляемые через `argparse`, сохраняются для возможности **ручной отладки и обработки ранее сохраненных файлов**, но основной рабочий процесс теперь инициируется через `scraper.py`.

---

#### **4. Взаимодействие компонентов**

Система спроектирована по принципу **разделения ответственности (Separation of Concerns)**.

1.  **Точка входа:** Пользователь запускает `scraper.py`.
2.  **`scraper.py`** инициализирует Selenium, настраивает "стелс"-параметры и начинает обход страниц.
3.  На каждой странице `scraper.py` получает HTML-код.
4.  **`scraper.py`** создает экземпляр класса `OfflineImporter` из `importer.py`.
5.  **`scraper.py`** вызывает метод `importer.process_html(html_content)`, передавая ему сырой HTML.
6.  **`importer.py`** полностью берет на себя дальнейшую работу: парсинг, эвристический анализ, запуск интерактивного CLI для валидации и, наконец, сохранение `.json` файла в `output/`.
7.  Цикл повторяется для следующего поста/страницы.

#### **5. Тестирование и Обеспечение Качества (Новый раздел)**

**5.1. Стратегия тестирования**

Ключевым компонентом системы, отвечающим за преобразование данных, является модуль `importer.py`. Его логика основана на эвристиках и CSS-селекторах, что делает его уязвимым к изменениям в верстке исходных HTML-страниц. Для обеспечения стабильности и надежности конвейера, **основной фокус тестирования направлен на модуль `importer.py`**.

Архитектурное разделение на `scraper.py` и `importer.py` позволяет проводить тестирование модуля обработки в полностью изолированной офлайн-среде, что делает тесты быстрыми, детерминированными и независимыми от состояния сети или целевого сайта.

**5.2. Требования к тестовому покрытию**

Необходимо создать исчерпывающий набор автоматизированных тестов для модуля `importer.py`, который будет запускаться перед каждым значительным изменением в коде для предотвращения регрессий.

*   **Инструменты:** Для написания и запуска тестов будет использоваться стандартный для индустрии фреймворк `pytest`.
*   **Структура:** Все тесты будут располагаться в директории `tests/`.

**5.3. Набор тестовых данных (Test Cases)**

Для отладки работы с различными вариантами данных будет создана папка `tests/fixtures/`, содержащая коллекцию эталонных HTML-файлов. Этот набор должен включать в себя как стандартные, так и крайние случаи (edge cases):

1.  **`ideal_post.html`**: "Идеальный" пост, содержащий все ожидаемые элементы: основной автор, ID поста, заголовок и описание в спойлерах, несколько вложений, включая как минимум один `.txt` файл. Это основной "happy path" сценарий.
2.  **`post_no_attachments.html`**: Пост без каких-либо прикрепленных файлов. Тест должен убедиться, что парсер корректно обрабатывает этот случай и возвращает пустой список вложений, не вызывая ошибок.
3.  **`post_no_main_author.html`**: Пост, где отсутствует основной блок с автором (`span.normalname`), но присутствует информация о последнем редактировании (`span.edit`). Тест должен проверить корректную работу **запасной эвристики** по поиску автора.
4.  **`post_multiple_txt.html`**: Пост с несколькими `.txt` вложениями в одной группе. Тест должен проверить логику приоритетной загрузки.
5.  **`post_only_spoilers.html`**: Пост, где контент представлен только в виде спойлеров, без отдельных файлов-вложений. Тест должен проверить, что каждый спойлер корректно определяется как отдельный `prompt_variant`.
6.  **`post_complex_formatting.html`**: Пост со сложной HTML-разметкой внутри (жирный текст, курсив, списки). Этот тест станет основой для **[Улучшения №1]** по конвертации в Markdown.
7.  **`post_broken_html.html`**: Файл с намеренно "сломанной" или неполной HTML-структурой. Тест должен гарантировать, что парсер не падает с критической ошибкой, а обрабатывает ситуацию gracefully (например, пропуская некорректный блок).

**5.4. Типы тестов**

*   **Модульные тесты (Unit Tests):** Будут созданы для отдельных, чистых функций внутри `importer.py` (например, для функции очистки HTML от тегов).
*   **Интеграционные тесты:** Основной тип тестов. Каждый тест будет принимать на вход один из эталонных HTML-файлов из `tests/fixtures/`, запускать полный цикл обработки в `OfflineImporter` (без интерактивной части) и сравнивать полученный на выходе структурированный объект данных с заранее подготовленным эталонным JSON-объектом. Это позволит проверять всю цепочку парсинга целиком.

**Преимущество такой архитектуры:** Модуль `importer.py` остается независимым. Его можно тестировать отдельно, "скармливая" ему локальные HTML-файлы, как и раньше. Модуль `scraper.py` выступает в роли "поставщика" данных, и его можно в будущем заменить на другой (например, использующий Playwright или другую технологию), не затрагивая ядро обработки.

