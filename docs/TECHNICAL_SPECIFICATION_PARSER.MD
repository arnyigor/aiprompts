Принято. Это итоговое, исчерпывающее техническое задание, которое объединяет все предыдущие анализы и ваши уточнения. Оно включает опциональную, но детально прописанную интеграцию с малыми локальными LLM, такими как Ollama, для усиления алгоритма.

---

## **Техническое Задание: Компонент `ForumPromptParser` (Гибридная модель с опциональной поддержкой LLM)**

### **1. Название компонента**
`ForumPromptParser`

### **2. Назначение**
Компонент предназначен для интеллектуального парсинга HTML-страниц форума с целью извлечения, классификации и структурирования данных о промптах для ИИ.

Архитектура компонента является **гибридной** и состоит из двух уровней:
1.  **Базовый движок (Rule-Based Engine):** Быстрый и надежный парсинг на основе детерминированных правил (с использованием Jsoup) для обработки большинства известных и структурированных форматов постов. **Этот движок является основным и должен работать полностью автономно.**
2.  **[Опционально] Вспомогательный модуль (LLM-Enhanced Module):** Локальная малая языковая модель (LLM, до 8B параметров, например, через Ollama) для решения семантически сложных задач, которые не поддаются формальным правилам: классификация неоднозначных постов, извлечение данных из неструктурированного текста и обогащение метаданных. **Этот модуль является опциональным улучшением.**

Итоговые данные должны быть преобразованы в коллекцию объектов, соответствующих JSON-структуре `PromptData`.

### **3. Функциональные требования**
3.1. **Входные данные:** Компонент должен принимать на вход HTML-содержимое страницы в виде `String`.
3.2. **Сегментация:** Должен производить разделение HTML на отдельные посты, используя CSS-селектор `table.ipbtable[data-post]`.
3.3. **Извлечение метаданных:** Для каждого поста должны извлекаться базовые метаданные: ID поста, автор, дата создания и дата последнего изменения (приоритетна).
3.4. **Классификация постов:** Компонент должен реализовывать алгоритм классификации для определения типа содержимого каждого поста. Минимальный набор типов:
- `META_PROMPT`, `EXTERNAL_RESOURCE`, `STANDARD_PROMPT`, `JAILBREAK`, `TEMPLATE_PROMPT`, `FILE_ATTACHMENT`, `DISCUSSION`.
3.5. **Стратегии извлечения:** В зависимости от типа поста, должна применяться соответствующая стратегия парсинга.
3.6. **Обработка вложений:** Компонент должен обнаруживать прикрепленные файлы.
- Если прикреплен `.txt` файл, его содержимое **приоритетно** используется как основной текст промпта. Текст из тела самого поста становится описанием.
- Информация о других файлах (`.zip` и т.д.) сохраняется, но их содержимое не обрабатывается.
3.7. **Формирование результата:** Результат парсинга должен быть преобразован в `List<PromptData>`. Поле `type` в `prompt_variants` по умолчанию должно иметь значение `"prompt"`.
3.8. **[Опционально] Обогащение данных с помощью LLM:** Если LLM-модуль активен, после основного парсинга компонент должен асинхронно вызывать LLM для:
- Генерации поля `tags` на основе `description` и `content`.
- Генерации краткой аннотации для поля `description`.

### **4. Нефункциональные требования**
4.1. **Производительность:** Обработка страницы должна выполняться асинхронно в фоновом потоке (`Dispatchers.IO`).
4.2. **Надежность:** Ошибки при парсинге одного поста не должны прерывать обработку остальных. Проблемные посты должны логироваться.
4.3. **Расширяемость:** Архитектура должна позволять легко добавлять новые `PostType` и `ParsingStrategy`.
4.4. **[Опционально (для LLM)] Управление зависимостями:** Компонент должен корректно функционировать, даже если LLM-сервис (например, Ollama) не запущен или недоступен. Зависимость от LLM должна быть опциональной и инкапсулированной.

### **5. Архитектура и структура кода**
-   **`PromptData.kt`**: `@Serializable data class`, соответствующий JSON-структуре.
-   **`PostType.kt`**: `enum class` со всеми типами постов.
-   **`ParsingStrategy.kt`**: `sealed interface` с методом `suspend fun parse(postElement: Element): PromptData?`.
-   **`PostClassifier.kt`**: Класс для определения `PostType`. Может иметь опциональную зависимость от `LLMService`.
-   **`ForumPromptParser.kt`**: Основной класс-оркестратор.
-   **`LLMService.kt`**: **Новый `interface`**, абстрагирующий все взаимодействия с LLM.
    -   `suspend fun classifyPost(textContent: String): PostType`
    -   `suspend fun extractDataFromText(textContent: String): ExtractedData`
    -   `suspend fun generateTags(text: String): List<String>`
-   **`OllamaLLMService.kt`**: Реализация `LLMService` для работы с локальным сервером Ollama.
-   **`NoOpLLMService.kt`**: "Пустая" реализация `LLMService`, которая не выполняет никаких действий и возвращает значения по умолчанию. Используется, когда LLM-модуль отключен.

### **6. Детальная реализация "Умного Алгоритма"**

#### **Шаг 1. Точка входа: метод `parse`**
Оркестрирует асинхронную обработку всех постов, как описано ранее.

#### **Шаг 2. Гибридная классификация поста (`processPost`)**
1.  **Извлечение метаданных:** Без изменений.
2.  **Классификация:** `PostClassifier` последовательно применяет быстрые правила.
    -   Если тип определен с высокой уверенностью (например, найден "ПРОМТ №"), классификация завершается.
    -   Если правила не сработали, и **если LLM-сервис сконфигурирован (не `NoOpLLMService`)**, классификатор вызывает `llmService.classifyPost(textContent)`. Промпт для LLM должен быть четким: *"Проанализируй текст и верни ОДНО из следующих значений: [список значений из PostType enum]"*.
    -   Если LLM недоступен или вернул ошибку, пост классифицируется как `DISCUSSION`.

#### **Шаг 3. Гибридные стратегии извлечения**
-   Для четко определенных типов (`STANDARD_PROMPT`, `FILE_ATTACHMENT` и др.) используются быстрые, детерминированные стратегии.
-   **`LLMExtractionStrategy` (новая стратегия):**
    -   Применяется к постам, которые LLM классифицировал как `TEMPLATE_PROMPT` или другой тип с неструктурированным текстом.
    -   Вызывает `llmService.extractDataFromText(textContent)`. Промпт должен содержать примеры (few-shot) для извлечения `title`, `description` и `prompt_content` в JSON-формате.

#### **Шаг 4. Финальное формирование и опциональное обогащение**
1.  После формирования `PromptData` любой из стратегий, проверяется, активен ли LLM-сервис.
2.  Если активен, запускается **неблокирующая фоновая задача** (`launch`), которая вызывает `llmService.generateTags()` для обогащения объекта `PromptData` тегами.

### **7. Интерфейсы и API**

```kotlin
@Serializable
data class PromptData( /* ... все поля согласно JSON ... */ )

// Интерфейс для взаимодействия с LLM
interface LLMService { /* ... методы classifyPost, extractDataFromText, generateTags ... */ }

// Основной интерфейс парсера
interface IForumPromptParser {
    suspend fun parse(htmlContent: String): List<PromptData>
}
```

### **8. Обработка ошибок**
-   **`LLMInferenceException`**: Новое исключение для ошибок, связанных с работой LLM.
-   **Graceful Degradation (Изящная деградация):** Если вызов LLM завершается ошибкой, компонент должен продолжить работу в режиме "без LLM": пост либо пропускается (классифицируется как `DISCUSSION`), либо возвращается без обогащенных данных.

### **9. Стратегия тестирования**
#### **9.1. Модульное тестирование (Unit Testing)**
-   **`LLMService`**: Тестируется с использованием `MockK`. Проверяется корректность формирования промптов и обработка ответов от мокированного LLM.
-   **`PostClassifier`**: Тесты должны проверять, что `LLMService` вызывается только в случае, когда быстрые правила не сработали.
-   **`ForumPromptParser`**: Тестируется с мокированными `PostClassifier` и `ParsingStrategy`.

#### **9.2. Интеграционное и Сквозное тестирование (E2E)**
-   Для E2E-тестов требуется запущенный локальный инстанс LLM (например, в Docker-контейнере с Ollama).
-   Создаются отдельные HTML-сэмплы для проверки работы LLM-стратегий на неструктурированном тексте.
-   **Тест на деградацию:** Должен быть тест, который проверяет, что `ForumPromptParser` корректно работает с `NoOpLLMService` и не падает, если реальный LLM-сервис недоступен.

### **10. Пример использования**

```kotlin
import kotlinx.coroutines.runBlocking

suspend fun main() {
    val useLlm = true // Флаг для включения/отключения LLM

    // 1. Инициализация зависимостей
    val llmService: LLMService = if (useLlm) {
        OllamaLLMService(httpClient = createHttpClient())
    } else {
        NoOpLLMService()
    }

    val parser: IForumPromptParser = createForumPromptParser(llmService) // DI-функция

    // 2. Загрузка HTML
    val htmlContent = loadHtmlFromFile("path/to/forum_page.html")

    // 3. Асинхронный запуск
    runBlocking {
        val prompts = parser.parse(htmlContent)
        println("Извлечено ${prompts.size} промптов.")

        // 4. Ожидание завершения фоновых задач обогащения (опционально)
        // ...
    }
}
```